{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_seq2seq_attn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**6. Seq2seq + Attention**\r\n",
        "1. 여러 Attention 모듈을 구현합니다.\r\n",
        "2. 기존 Seq2seq 모델과의 차이를 이해합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOoDGkaFkrd2"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from torch import nn\r\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "import torch\r\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz8nkrRZSysK"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DBRVAT32YEw"
      },
      "source": [
        "데이터 처리는 이전과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1neCRvux8k6Z"
      },
      "source": [
        "vocab_size = 100\r\n",
        "pad_id = 0\r\n",
        "sos_id = 1\r\n",
        "eos_id = 2\r\n",
        "\r\n",
        "src_data = [\r\n",
        "  [3, 77, 56, 26, 3, 55, 12, 36, 31],\r\n",
        "  [58, 20, 65, 46, 26, 10, 76, 44],\r\n",
        "  [58, 17, 8],\r\n",
        "  [59],\r\n",
        "  [29, 3, 52, 74, 73, 51, 39, 75, 19],\r\n",
        "  [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93],\r\n",
        "  [39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99, 5],\r\n",
        "  [75, 34, 17, 3, 86, 88],\r\n",
        "  [63, 39, 5, 35, 67, 56, 68, 89, 55, 66],\r\n",
        "  [12, 40, 69, 39, 49]\r\n",
        "]\r\n",
        "\r\n",
        "trg_data = [\r\n",
        "  [75, 13, 22, 77, 89, 21, 13, 86, 95],\r\n",
        "  [79, 14, 91, 41, 32, 79, 88, 34, 8, 68, 32, 77, 58, 7, 9, 87],\r\n",
        "  [85, 8, 50, 30],\r\n",
        "  [47, 30],\r\n",
        "  [8, 85, 87, 77, 47, 21, 23, 98, 83, 4, 47, 97, 40, 43, 70, 8, 65, 71, 69, 88],\r\n",
        "  [32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18],\r\n",
        "  [37, 14, 49, 24, 93, 37, 54, 51, 39, 84],\r\n",
        "  [16, 98, 68, 57, 55, 46, 66, 85, 18],\r\n",
        "  [20, 70, 14, 6, 58, 90, 30, 17, 91, 18, 90],\r\n",
        "  [37, 93, 98, 13, 45, 28, 89, 72, 70]\r\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwq5SNGUdCT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173fda40-6633-4355-a71b-d839cd6dd297"
      },
      "source": [
        "trg_data = [[sos_id]+seq+[eos_id] for seq in tqdm(trg_data)]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 30817.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSeExSrRAYg8"
      },
      "source": [
        "def padding(data, is_src=True):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  valid_lens = []\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    valid_lens.append(len(seq))\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, valid_lens, max_len"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCXaXdk-ApJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e85c370-2362-4427-f5ad-7631f2221886"
      },
      "source": [
        "src_data, src_lens, src_max_len = padding(src_data)\r\n",
        "trg_data, trg_lens, trg_max_len = padding(trg_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 19337.50it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 42238.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 15\n",
            "Maximum sequence length: 22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F3Mx8pbAvqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b87fdf-7703-44c9-e6a1-0de5acc90e64"
      },
      "source": [
        "# B: batch size, S_L: source maximum sequence length, T_L: target maximum sequence length\r\n",
        "src_batch = torch.LongTensor(src_data)  # (B, S_L)\r\n",
        "src_batch_lens = torch.LongTensor(src_lens)  # (B)\r\n",
        "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\r\n",
        "trg_batch_lens = torch.LongTensor(trg_lens)  # (B)\r\n",
        "\r\n",
        "print(src_batch.shape)\r\n",
        "print(src_batch_lens.shape)\r\n",
        "print(trg_batch.shape)\r\n",
        "print(trg_batch_lens.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 15])\n",
            "torch.Size([10])\n",
            "torch.Size([10, 22])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxmvrpQABWn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4bf114-c1e2-42c9-a8e9-2571fb1dada0"
      },
      "source": [
        "src_batch_lens, sorted_idx = src_batch_lens.sort(descending=True)\r\n",
        "src_batch = src_batch[sorted_idx]\r\n",
        "trg_batch = trg_batch[sorted_idx]\r\n",
        "trg_batch_lens = trg_batch_lens[sorted_idx]\r\n",
        "\r\n",
        "print(src_batch)\r\n",
        "print(src_batch_lens)\r\n",
        "print(trg_batch)\r\n",
        "print(trg_batch_lens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99,  5],\n",
            "        [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93,  0,  0,  0,  0],\n",
            "        [63, 39,  5, 35, 67, 56, 68, 89, 55, 66,  0,  0,  0,  0,  0],\n",
            "        [ 3, 77, 56, 26,  3, 55, 12, 36, 31,  0,  0,  0,  0,  0,  0],\n",
            "        [29,  3, 52, 74, 73, 51, 39, 75, 19,  0,  0,  0,  0,  0,  0],\n",
            "        [58, 20, 65, 46, 26, 10, 76, 44,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [75, 34, 17,  3, 86, 88,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [12, 40, 69, 39, 49,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [58, 17,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [59,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "tensor([15, 11, 10,  9,  9,  8,  6,  5,  3,  1])\n",
            "tensor([[ 1, 37, 14, 49, 24, 93, 37, 54, 51, 39, 84,  2,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18,  2,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 20, 70, 14,  6, 58, 90, 30, 17, 91, 18, 90,  2,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 75, 13, 22, 77, 89, 21, 13, 86, 95,  2,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1,  8, 85, 87, 77, 47, 21, 23, 98, 83,  4, 47, 97, 40, 43, 70,  8, 65,\n",
            "         71, 69, 88,  2],\n",
            "        [ 1, 79, 14, 91, 41, 32, 79, 88, 34,  8, 68, 32, 77, 58,  7,  9, 87,  2,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 16, 98, 68, 57, 55, 46, 66, 85, 18,  2,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 37, 93, 98, 13, 45, 28, 89, 72, 70,  2,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 85,  8, 50, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0],\n",
            "        [ 1, 47, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0]])\n",
            "tensor([12, 14, 13, 11, 22, 18, 11, 11,  6,  4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emD3bFjS2vEn"
      },
      "source": [
        "### **Encoder 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5k9sSui29yP"
      },
      "source": [
        "Encoder 역시 기존 Seq2seq 모델과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmhCME-PDUJ8"
      },
      "source": [
        "embedding_size = 256\r\n",
        "hidden_size = 512\r\n",
        "num_layers = 2\r\n",
        "num_dirs = 2\r\n",
        "dropout = 0.1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epZDaDO-FMPu"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n",
        "    self.gru = nn.GRU(\r\n",
        "        input_size=embedding_size, \r\n",
        "        hidden_size=hidden_size,\r\n",
        "        num_layers=num_layers,\r\n",
        "        bidirectional=True if num_dirs > 1 else False,\r\n",
        "        dropout=dropout\r\n",
        "    )\r\n",
        "    self.linear = nn.Linear(num_dirs * hidden_size, hidden_size)\r\n",
        "\r\n",
        "  def forward(self, batch, batch_lens):  # batch: (B, S_L), batch_lens: (B)\r\n",
        "    # d_w: word embedding size\r\n",
        "    batch_emb = self.embedding(batch)  # (B, S_L, d_w)\r\n",
        "    batch_emb = batch_emb.transpose(0, 1)  # (S_L, B, d_w)\r\n",
        "\r\n",
        "    packed_input = pack_padded_sequence(batch_emb, batch_lens)\r\n",
        "\r\n",
        "    h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers*num_dirs, B, d_h) = (4, B, d_h)\r\n",
        "    packed_outputs, h_n = self.gru(packed_input, h_0)  # h_n: (4, B, d_h)\r\n",
        "    outputs = pad_packed_sequence(packed_outputs)[0]  # outputs: (S_L, B, 2d_h)\r\n",
        "    outputs = torch.tanh(self.linear(outputs))  # (S_L, B, d_h)\r\n",
        "\r\n",
        "    forward_hidden = h_n[-2, :, :]\r\n",
        "    backward_hidden = h_n[-1, :, :]\r\n",
        "    hidden = torch.tanh(self.linear(torch.cat((forward_hidden, backward_hidden), dim=-1))).unsqueeze(0)  # (1, B, d_h)\r\n",
        "\r\n",
        "    return outputs, hidden"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEdSnKZkIedk"
      },
      "source": [
        "encoder = Encoder()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w5G0uy4TiFA"
      },
      "source": [
        "### **Dot-product Attention 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-sPMEBEcRqP"
      },
      "source": [
        "우선 대표적인 attention 형태 중 하나인 Dot-product Attention은 다음과 같이 구현할 수 있습니다.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEB-og7IcYN6"
      },
      "source": [
        "class DotAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "  def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)\r\n",
        "    query = decoder_hidden.squeeze(0)  # (B, d_h)\r\n",
        "    key = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\r\n",
        "\r\n",
        "    energy = torch.sum(torch.mul(key, query.unsqueeze(1)), dim=-1)  # (B, S_L)\r\n",
        "\r\n",
        "    attn_scores = F.softmax(energy, dim=-1)  # (B, S_L)\r\n",
        "    attn_values = torch.sum(torch.mul(encoder_outputs.transpose(0, 1), attn_scores.unsqueeze(2)), dim=1)  # (B, d_h)\r\n",
        "\r\n",
        "    return attn_values, attn_scores"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIARwx4IjuuG"
      },
      "source": [
        "dot_attn = DotAttention()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r94WCkbCjMnz"
      },
      "source": [
        "이제 이 attention 모듈을 가지는 Decoder 클래스를 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JycRs0ojLyg"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self, attention):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n",
        "    self.attention = attention\r\n",
        "    self.rnn = nn.GRU(\r\n",
        "        embedding_size,\r\n",
        "        hidden_size\r\n",
        "    )\r\n",
        "    self.output_linear = nn.Linear(2*hidden_size, vocab_size)\r\n",
        "\r\n",
        "  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (L, B, d_h), hidden: (1, B, d_h)  \r\n",
        "    batch_emb = self.embedding(batch)  # (B, d_w)\r\n",
        "    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\r\n",
        "\r\n",
        "    outputs, hidden = self.rnn(batch_emb, hidden)  # (1, B, d_h), (1, B, d_h)\r\n",
        "\r\n",
        "    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\r\n",
        "    concat_outputs = torch.cat((outputs, attn_values.unsqueeze(0)), dim=-1)  # (1, B, 2d_h)\r\n",
        "\r\n",
        "    return self.output_linear(concat_outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45GG2CvOjwzE"
      },
      "source": [
        "decoder = Decoder(dot_attn)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ1NzYZROrOu"
      },
      "source": [
        "### **Seq2seq 모델 구축**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEYvsQS0Ovp6"
      },
      "source": [
        "최종적으로 seq2seq 모델을 다음과 같이 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52xKNVeF37N"
      },
      "source": [
        "class Seq2seq(nn.Module):\r\n",
        "  def __init__(self, encoder, decoder):\r\n",
        "    super(Seq2seq, self).__init__()\r\n",
        "\r\n",
        "    self.encoder = encoder\r\n",
        "    self.decoder = decoder\r\n",
        "\r\n",
        "  def forward(self, src_batch, src_batch_lens, trg_batch, teacher_forcing_prob=0.5):\r\n",
        "    # src_batch: (B, S_L), src_batch_lens: (B), trg_batch: (B, T_L)\r\n",
        "\r\n",
        "    encoder_outputs, hidden = self.encoder(src_batch, src_batch_lens)  # encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)\r\n",
        "\r\n",
        "    input_ids = trg_batch[:, 0]  # (B)\r\n",
        "    batch_size = src_batch.shape[0]\r\n",
        "    outputs = torch.zeros(trg_max_len, batch_size, vocab_size)  # (T_L, B, V)\r\n",
        "\r\n",
        "    for t in range(1, trg_max_len):\r\n",
        "      decoder_outputs, hidden = self.decoder(input_ids, encoder_outputs, hidden)  # decoder_outputs: (B, V), hidden: (1, B, d_h)\r\n",
        "\r\n",
        "      outputs[t] = decoder_outputs\r\n",
        "      _, top_ids = torch.max(decoder_outputs, dim=-1)  # top_ids: (B)\r\n",
        "\r\n",
        "      input_ids = trg_batch[:, t] if random.random() > teacher_forcing_prob else top_ids\r\n",
        "\r\n",
        "    return outputs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNv7wlRgPIYS"
      },
      "source": [
        "seq2seq = Seq2seq(encoder, decoder)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFwbnxd7PVNf"
      },
      "source": [
        "### **모델 사용해보기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIprc5N2jaV2"
      },
      "source": [
        "만든 모델로 결과를 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKdTDHqsiLbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4107db-2109-4abf-93ea-0965ae66b9d4"
      },
      "source": [
        "# V: vocab size\r\n",
        "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)  # (T_L, B, V)\r\n",
        "\r\n",
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 7.8578e-02, -3.9708e-02,  2.4983e-02,  ...,  5.9400e-02,\n",
            "           5.8773e-02,  1.5659e-02],\n",
            "         [ 1.2061e-01, -3.1487e-02,  2.3225e-03,  ...,  1.4981e-02,\n",
            "           3.3029e-02,  5.0413e-02],\n",
            "         [ 8.6072e-02, -2.7037e-03, -4.5105e-02,  ...,  4.0447e-02,\n",
            "           6.3354e-02,  4.1896e-02],\n",
            "         ...,\n",
            "         [ 7.8040e-02, -4.3367e-02, -1.0225e-02,  ...,  6.8456e-02,\n",
            "           7.2114e-02,  1.3307e-02],\n",
            "         [ 9.3360e-02, -2.9205e-02, -3.3500e-03,  ...,  2.6562e-02,\n",
            "           8.5536e-02,  2.4281e-03],\n",
            "         [ 9.3738e-02, -4.1302e-02, -1.4421e-02,  ...,  6.9021e-02,\n",
            "           6.2003e-02,  2.0697e-02]],\n",
            "\n",
            "        [[ 1.7220e-01,  2.5173e-02,  6.6813e-02,  ...,  4.9789e-02,\n",
            "           4.6872e-02,  7.8386e-02],\n",
            "         [ 6.7188e-03,  1.1459e-02, -4.7066e-02,  ...,  2.1594e-02,\n",
            "           4.9326e-02,  3.1331e-02],\n",
            "         [-2.3417e-02, -1.8742e-02,  4.0032e-03,  ..., -2.4672e-03,\n",
            "           1.0938e-01,  3.6957e-02],\n",
            "         ...,\n",
            "         [ 1.7101e-01,  2.5021e-02,  3.2180e-02,  ...,  5.7410e-02,\n",
            "           5.8349e-02,  7.7221e-02],\n",
            "         [-2.3478e-02, -1.9729e-01,  6.2772e-02,  ...,  1.0165e-01,\n",
            "           1.2747e-01,  1.2618e-01],\n",
            "         [ 2.5574e-02, -1.4922e-02, -5.8787e-02,  ...,  1.1005e-02,\n",
            "           1.4454e-02,  7.5784e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.1451e-02, -3.2476e-02,  1.8324e-02,  ...,  5.7740e-02,\n",
            "           1.4419e-01, -8.0592e-02],\n",
            "         [ 2.5365e-02, -1.8626e-01,  2.2191e-01,  ..., -8.3784e-02,\n",
            "           4.8703e-02,  6.3890e-02],\n",
            "         [-5.4770e-02, -4.1824e-02,  1.4147e-02,  ...,  6.0573e-02,\n",
            "           1.2858e-01, -8.5019e-02],\n",
            "         ...,\n",
            "         [-2.2901e-02, -3.1123e-02, -1.1474e-02,  ...,  5.7177e-02,\n",
            "           1.5369e-01, -9.3788e-02],\n",
            "         [-2.6667e-02, -3.3246e-02, -3.2699e-03,  ...,  6.0245e-02,\n",
            "           1.4804e-01, -9.4308e-02],\n",
            "         [-1.7472e-02, -3.9348e-02, -2.2765e-03,  ...,  5.9694e-02,\n",
            "           1.5430e-01, -9.3301e-02]],\n",
            "\n",
            "        [[-2.8789e-02, -2.1523e-02, -6.0298e-02,  ..., -4.8320e-03,\n",
            "           1.0668e-01,  7.0233e-02],\n",
            "         [ 7.7370e-04,  9.8570e-03,  6.0417e-02,  ...,  1.7491e-02,\n",
            "          -8.2915e-02,  1.5548e-01],\n",
            "         [-6.7649e-02, -7.4838e-03, -8.2164e-02,  ...,  4.3390e-03,\n",
            "           9.3610e-02,  6.9857e-02],\n",
            "         ...,\n",
            "         [-3.9015e-02, -1.9346e-02, -9.2144e-02,  ..., -8.3444e-03,\n",
            "           1.1552e-01,  5.6324e-02],\n",
            "         [-3.8808e-02, -2.3932e-02, -8.2561e-02,  ..., -4.4236e-03,\n",
            "           1.1272e-01,  5.6560e-02],\n",
            "         [-3.2250e-02, -2.8682e-02, -8.1960e-02,  ..., -4.7456e-03,\n",
            "           1.1673e-01,  5.7487e-02]],\n",
            "\n",
            "        [[-2.2540e-02, -6.3463e-03, -7.7750e-02,  ..., -4.6235e-02,\n",
            "           8.8288e-02,  1.5271e-01],\n",
            "         [-2.6632e-02, -1.1184e-02,  2.3241e-02,  ...,  1.1103e-01,\n",
            "           4.1058e-02,  9.4863e-02],\n",
            "         [-7.8036e-02,  5.7143e-02, -3.7789e-02,  ...,  3.9788e-02,\n",
            "           5.3009e-02,  7.6823e-05],\n",
            "         ...,\n",
            "         [-5.6292e-02,  4.5338e-02, -3.6860e-02,  ...,  2.7867e-02,\n",
            "           6.7627e-02, -1.3651e-02],\n",
            "         [-5.2309e-02, -5.6651e-02, -2.5194e-01,  ..., -9.5254e-02,\n",
            "           1.3172e-01,  5.7246e-02],\n",
            "         [-5.0934e-02,  3.8057e-02, -2.8702e-02,  ...,  3.0142e-02,\n",
            "           6.7969e-02, -1.2301e-02]]], grad_fn=<CopySlices>)\n",
            "torch.Size([22, 10, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-wAEwi9dy0Q"
      },
      "source": [
        "sample_sent = [4, 10, 88, 46, 72, 34, 14, 51]\r\n",
        "sample_len = len(sample_sent)\r\n",
        "\r\n",
        "sample_batch = torch.LongTensor(sample_sent).unsqueeze(0)  # (1, L)\r\n",
        "sample_batch_len = torch.LongTensor([sample_len])  # (1)\r\n",
        "\r\n",
        "encoder_output, hidden = seq2seq.encoder(sample_batch, sample_batch_len)  # hidden: (4, 1, d_h)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ywRSK1iTn1U"
      },
      "source": [
        "input_id = torch.LongTensor([sos_id]) # (1)\r\n",
        "output = []\r\n",
        "\r\n",
        "for t in range(1, trg_max_len):\r\n",
        "  decoder_output, hidden = seq2seq.decoder(input_id, encoder_output, hidden)  # decoder_output: (1, V), hidden: (4, 1, d_h)\r\n",
        "\r\n",
        "  _, top_id = torch.max(decoder_output, dim=-1)  # top_ids: (1)\r\n",
        "\r\n",
        "  if top_id == eos_id:\r\n",
        "    break\r\n",
        "  else:\r\n",
        "    output += top_id.tolist()\r\n",
        "    input_id = top_id"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP_A4ZrhTXik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9485bde7-1090-421b-bc37-74f6ea68898c"
      },
      "source": [
        "output"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[69,\n",
              " 47,\n",
              " 16,\n",
              " 14,\n",
              " 25,\n",
              " 38,\n",
              " 60,\n",
              " 65,\n",
              " 60,\n",
              " 10,\n",
              " 81,\n",
              " 39,\n",
              " 39,\n",
              " 39,\n",
              " 78,\n",
              " 16,\n",
              " 52,\n",
              " 50,\n",
              " 39,\n",
              " 39,\n",
              " 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4TZfceq3Nbs"
      },
      "source": [
        "### **Concat Attention 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxpAQjm3Y9U"
      },
      "source": [
        "Bahdanau Attention이라고도 불리는 Concat Attention을 구현해보도록 하겠습니다.  \r\n",
        "\r\n",
        "\r\n",
        "*   `self.w`: Concat한 query와 key 벡터를 1차적으로 linear transformation.\r\n",
        "*   `self.v`: Attention logit 값을 계산.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHRfFeIzJJU7"
      },
      "source": [
        "class ConcatAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.w = nn.Linear(2*hidden_size, hidden_size, bias=False)\r\n",
        "    self.v = nn.Linear(hidden_size, 1, bias=False)\r\n",
        "\r\n",
        "  def forward(self, decoder_hidden, encoder_outputs):  # (1, B, d_h), (S_L, B, d_h)\r\n",
        "    src_max_len = encoder_outputs.shape[0]\r\n",
        "\r\n",
        "    decoder_hidden = decoder_hidden.transpose(0, 1).repeat(1, src_max_len, 1)  # (B, S_L, d_h)\r\n",
        "    encoder_outputs = encoder_outputs.transpose(0, 1)  # (B, S_L, d_h)\r\n",
        "\r\n",
        "    concat_hiddens = torch.cat((decoder_hidden, encoder_outputs), dim=2)  # (B, S_L, 2d_h)\r\n",
        "    energy = torch.tanh(self.w(concat_hiddens))  # (B, S_L, d_h)\r\n",
        "\r\n",
        "    attn_scores = F.softmax(self.v(energy), dim=1)  # (B, S_L, 1)\r\n",
        "    attn_values = torch.sum(torch.mul(encoder_outputs, attn_scores), dim=1)  # (B, d_h)\r\n",
        "\r\n",
        "    return attn_values, attn_scores"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utm4b5uzNS40"
      },
      "source": [
        "concat_attn = ConcatAttention()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBBCV9G-M1cw"
      },
      "source": [
        "마찬가지로 decoder를 마저 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnppmsXNSaGP"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self, attention):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n",
        "    self.attention = attention\r\n",
        "    self.rnn = nn.GRU(\r\n",
        "        embedding_size + hidden_size,\r\n",
        "        hidden_size\r\n",
        "    )\r\n",
        "    self.output_linear = nn.Linear(hidden_size, vocab_size)\r\n",
        "\r\n",
        "  def forward(self, batch, encoder_outputs, hidden):  # batch: (B), encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)  \r\n",
        "    batch_emb = self.embedding(batch)  # (B, d_w)\r\n",
        "    batch_emb = batch_emb.unsqueeze(0)  # (1, B, d_w)\r\n",
        "\r\n",
        "    attn_values, attn_scores = self.attention(hidden, encoder_outputs)  # (B, d_h), (B, S_L)\r\n",
        "\r\n",
        "    concat_emb = torch.cat((batch_emb, attn_values.unsqueeze(0)), dim=-1)  # (1, B, d_w+d_h)\r\n",
        "\r\n",
        "    outputs, hidden = self.rnn(concat_emb, hidden)  # (1, B, d_h), (1, B, d_h)\r\n",
        "\r\n",
        "    return self.output_linear(outputs).squeeze(0), hidden  # (B, V), (1, B, d_h)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gA4GJqgOoMT"
      },
      "source": [
        "decoder = Decoder(concat_attn)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQI9J0VGj4cc"
      },
      "source": [
        "seq2seq = Seq2seq(encoder, decoder)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6suDtTxj4ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092fc0b7-b0fa-478f-a528-214e61b5bd0b"
      },
      "source": [
        "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)\r\n",
        "\r\n",
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 1.0623e-01, -6.1548e-02,  4.8492e-02,  ..., -3.1328e-02,\n",
            "          -1.3685e-01, -1.0746e-01],\n",
            "         [ 9.7381e-02, -4.8924e-02,  2.6773e-02,  ..., -3.1588e-02,\n",
            "          -5.4356e-02, -6.7176e-02],\n",
            "         [ 7.5696e-02, -2.2034e-02,  2.6381e-02,  ..., -4.7442e-02,\n",
            "          -1.0494e-01, -1.7339e-01],\n",
            "         ...,\n",
            "         [ 1.7933e-02, -5.6533e-02,  2.7698e-02,  ..., -2.9162e-02,\n",
            "          -1.1593e-01, -9.6979e-02],\n",
            "         [ 9.2278e-02, -4.4912e-02, -3.2735e-03,  ..., -5.8754e-02,\n",
            "          -1.0069e-01, -9.9915e-02],\n",
            "         [ 7.1182e-02, -2.5934e-02,  1.5830e-02,  ..., -3.0231e-02,\n",
            "          -1.0326e-01, -1.2691e-01]],\n",
            "\n",
            "        [[ 2.4493e-01,  3.1317e-02, -1.3265e-01,  ..., -1.1221e-02,\n",
            "          -9.0079e-02,  1.0479e-01],\n",
            "         [ 6.2757e-02,  2.2987e-01, -8.8775e-03,  ...,  3.1032e-02,\n",
            "           1.5697e-01, -3.1577e-02],\n",
            "         [ 3.2252e-02,  1.1773e-02, -1.6103e-01,  ...,  4.0531e-02,\n",
            "          -4.4196e-02,  1.7145e-03],\n",
            "         ...,\n",
            "         [ 1.8196e-01,  2.9974e-02, -1.4491e-01,  ...,  1.0823e-02,\n",
            "          -8.5803e-02,  9.4588e-02],\n",
            "         [ 2.4543e-01, -1.4200e-01, -1.0555e-01,  ..., -2.4558e-02,\n",
            "          -7.6972e-02, -1.1875e-01],\n",
            "         [ 1.2345e-01,  3.8936e-02, -1.2829e-01,  ...,  7.8941e-02,\n",
            "          -1.4480e-01, -1.8600e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.7676e-01,  7.2210e-02, -3.7351e-02,  ..., -2.0037e-01,\n",
            "           1.6591e-01, -2.3706e-01],\n",
            "         [ 1.6075e-01,  8.5691e-02, -2.1373e-02,  ..., -1.5855e-01,\n",
            "           1.5158e-01, -2.6338e-01],\n",
            "         [ 1.7154e-01,  7.3943e-02, -3.2403e-02,  ..., -1.5643e-01,\n",
            "           1.5094e-01, -2.6212e-01],\n",
            "         ...,\n",
            "         [ 1.4937e-01,  7.7039e-02, -3.6556e-02,  ..., -1.5684e-01,\n",
            "           1.5695e-01, -2.5382e-01],\n",
            "         [ 1.7082e-01,  6.7716e-02, -3.6571e-02,  ..., -1.7176e-01,\n",
            "           1.5864e-01, -2.4252e-01],\n",
            "         [ 1.6301e-01,  7.4217e-02, -3.1706e-02,  ..., -1.6968e-01,\n",
            "           1.5247e-01, -2.4036e-01]],\n",
            "\n",
            "        [[ 3.0155e-01, -5.8129e-02, -3.2732e-02,  ..., -2.5665e-01,\n",
            "           6.7274e-02, -2.2458e-01],\n",
            "         [ 2.8931e-01, -4.0172e-02, -1.9625e-02,  ..., -2.2203e-01,\n",
            "           5.9511e-02, -2.4396e-01],\n",
            "         [ 2.9849e-01, -5.2448e-02, -3.5252e-02,  ..., -2.1222e-01,\n",
            "           4.8339e-02, -2.4789e-01],\n",
            "         ...,\n",
            "         [ 2.7393e-01, -5.4221e-02, -3.4809e-02,  ..., -2.1531e-01,\n",
            "           5.6901e-02, -2.3951e-01],\n",
            "         [ 2.9183e-01, -6.3669e-02, -3.3892e-02,  ..., -2.2424e-01,\n",
            "           6.1778e-02, -2.3053e-01],\n",
            "         [ 2.8356e-01, -5.5918e-02, -2.8675e-02,  ..., -2.2212e-01,\n",
            "           5.5877e-02, -2.2883e-01]],\n",
            "\n",
            "        [[ 4.4983e-02, -1.2168e-01, -2.3910e-01,  ..., -2.3140e-01,\n",
            "           1.2851e-01,  8.6331e-03],\n",
            "         [ 3.3103e-02, -9.7359e-02, -2.2880e-01,  ..., -2.0138e-01,\n",
            "           1.1823e-01,  4.6111e-03],\n",
            "         [ 4.3189e-02, -1.1022e-01, -2.4438e-01,  ..., -1.9247e-01,\n",
            "           1.0545e-01, -5.7605e-03],\n",
            "         ...,\n",
            "         [ 2.1393e-02, -1.2002e-01, -2.3929e-01,  ..., -1.9361e-01,\n",
            "           1.1314e-01, -1.9516e-04],\n",
            "         [ 3.1166e-02, -1.2513e-01, -2.3659e-01,  ..., -2.0012e-01,\n",
            "           1.1924e-01,  7.5488e-03],\n",
            "         [ 2.4091e-02, -1.1857e-01, -2.3140e-01,  ..., -1.9825e-01,\n",
            "           1.1393e-01,  8.7424e-03]]], grad_fn=<CopySlices>)\n",
            "torch.Size([22, 10, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}