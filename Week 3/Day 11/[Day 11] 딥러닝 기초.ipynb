{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [AI Math 8강] 베이즈 통계학 맛보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조건부 확률\n",
    "- 베이즈 통계학을 이해하기 위해선 조건부 확률의 개념을 이해해야 한다\n",
    "- [베이즈 정리](https://ko.wikipedia.org/wiki/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%A0%95%EB%A6%AC)는 조건부 확률을 이용하여 정보를 갱신하는 방법을 알려준다\n",
    "<br><br>\n",
    "$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "<br>\n",
    "$$P(A\\cap B) = P(B)P(A|B)$$\n",
    "<br>\n",
    "$$P(B|A) = \\frac{P(A\\cap B)}{P(A)} = P(B)\\frac{P(A|B)}{P(A)}$$\n",
    "<br><br>\n",
    "- 베이즈 정리\n",
    "$$P(\\theta|\\mathcal D) = P(\\theta)\\frac{P(\\mathcal D|\\theta)}{P(\\mathcal D)}$$\n",
    "<br>\n",
    "\n",
    "    - 사후확률 (posterior)\n",
    "        $$P(\\theta|\\mathcal D)$$\n",
    "    - 사전확률 (prior)\n",
    "        $$P(\\theta)$$\n",
    "    - 가능도 (likelihood)\n",
    "        $$P(\\mathcal D|\\theta)$$\n",
    "    - Evidence\n",
    "        $$P(\\mathcal D)$$\n",
    "        \n",
    "        - if discrete random variable\n",
    "            $$P(\\mathcal D) = \\sum_\\theta^{}P(\\mathcal D|\\theta)P(\\theta)$$\n",
    "        - if continuous random variable\n",
    "            $$P(\\mathcal D) = \\int_{\\theta}^{}P(\\mathcal D|\\theta)P(\\theta)$$\n",
    "<br><br>\n",
    "- 조건부 확률의 시각화\n",
    "- 베이즈 정리를 통한 정보의 갱신\n",
    "    - 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있다\n",
    "- 조건부 확률 -> 인과관계 ?\n",
    "    - 조건부 확률은 유용한 통계적 해석을 제공하지만 인과관계(causality)를 추론할 때 함부로 사용해서는 안된다\n",
    "    - 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요하다\n",
    "        - 단, 인과관계만으로는 높은 예측 정확도를 담보하기는 어렵다\n",
    "    - 인과관계를 알아내기 위해서는 중첩요인(confoundingfactor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 한다\n",
    "        - 만일 중첩요인의 효과를 제거하지 않으면 가짜 연관성(spuriouscorrelation)이 나온다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [DL Basic] 딥러닝 기본 용어 설명 - Historical Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What make you a good deep learner?\n",
    "1. Implementation Skills\n",
    "2. Math Skills (Linear Algebra, Probability)\n",
    "3. Knowing a lot of recent Papers\n",
    "\n",
    "### Key Components of Deel Learning\n",
    "- The **data** that the model can learn from\n",
    "    - Image, Text, ...\n",
    "- The **model** how to transform the data\n",
    "    - AlexNet, GoogLeNet, ResNet, DenseNet, LSTM, Deep AutoEncoders, GAN, ...\n",
    "- The **loss function** that quantifies the badness of the model\n",
    "    - Regression Task\n",
    "        $$\\mathrm M\\mathrm S\\mathrm E = \\frac{1}{N}\\sum_{i=1}^N\\sum_{d=1}^D ({y_i^{(d)} - \\widehat{y}_i^{(d)}})^{2}$$\n",
    "    - Classification Task\n",
    "        $$\\mathrm C\\mathrm E = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{d=1}^Dy_i^{(d)}\\log\\widehat{y}_i^{(d)}$$\n",
    "    - Probabilistic Task\n",
    "        $$\\mathrm M\\mathrm L\\mathrm E = \\frac{1}{N}\\sum_{i=1}^N\\sum_{d=1}^D \\log\\mathcal N(y_i^{(d)};\\widehat{y}_i^{(d)},1)\\qquad(=\\mathrm M\\mathrm S\\mathrm E)$$\n",
    "- The **algorithm** to adjust the parameters to minimize the loss\n",
    "    - Dropout\n",
    "    - Early stopping\n",
    "    - k-fold validation\n",
    "    - Weight decay\n",
    "    - Batch normalization\n",
    "    - MixUp\n",
    "    - Ensemble\n",
    "    - Bayesian Optimization\n",
    "\n",
    "### Historical Review\n",
    "[Deep Learning's Most Important Ideas - A Brief Historical Review](https://dennybritz.com/blog/deep-learning-most-important-ideas.pdf) by [Denny Britz](https://dennybritz.com/about) (2020-07-29)\n",
    "- 2012 - [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "- 2013 - [DQN](https://arxiv.org/pdf/1312.5602.pdf)\n",
    "- 2014 - [Encoder / Decoder](https://arxiv.org/pdf/1406.1078.pdf)\n",
    "- 2014 - [Adam Optimizer](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "- 2015 - [Generative Adversarial Network (GAN)](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)\n",
    "- 2015 - [Residual Networks](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- 2017 - [Transformer](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- 2018 - [BERT (fine-tuned NLP models)](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- 2019 - BIG Language Models\n",
    "    - [GPT1 - Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "    - [GPT2 - Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "    - [**GPT3 - Language Models are Few-Shot Learners**](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "- 2020 - Self Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [DL Basic] [PyTorch 시작하기](https://github.com/changwoomon/Boostcamp-AI-Tech/blob/main/Week%203/Day%2011/torch_basics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [DL Basic] [뉴럴 네트워크 - MLP](https://github.com/changwoomon/Boostcamp-AI-Tech/blob/main/Week%203/Day%2011/mlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [DL Basic] [데이터셋 다루기](https://github.com/changwoomon/Boostcamp-AI-Tech/blob/main/Week%203/Day%2011/dataset_example.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
